{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51DZVX72bhOi"
      },
      "source": [
        "## Assignment 2.1: Text classification via RNN (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMQG-uBzbhOo"
      },
      "source": [
        "In this assignment you will perform sentiment analysis of the IMDBs reviews by using RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3rycNfRbhOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8a5ed6-b156-4b07-9025-4402f6903d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: torchtext==0.7 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (0.1.96)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (1.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (1.21.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.7) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.6.0\n",
        "!pip install torchtext==0.7\n",
        "!pip install numpy\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4DiQ5cqbhOq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torchtext import datasets\n",
        "\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import BucketIterator\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9LiMmPHbhOq"
      },
      "source": [
        "### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut8z3McSbhOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5981a49-6c09-44be-e2d1-8139a614afa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ],
      "source": [
        "TEXT = Field(sequential=True, lower=True)\n",
        "LABEL = LabelField()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6_zyv4pbhOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73a16ee-8b43-4b24-8913-623b86131c88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ]
        }
      ],
      "source": [
        "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
        "trn, vld = train.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HYQal-7bhOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161bcc55-f083-4370-f1a3-d93ad483f3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.19 s, sys: 36.5 ms, total: 1.23 s\n",
            "Wall time: 1.23 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "TEXT.build_vocab(trn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5cRhX1-bhOt"
      },
      "outputs": [],
      "source": [
        "LABEL.build_vocab(trn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zQGE95SbhOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a5f91f-bc7d-415c-ab30-7631c4514287"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 225710),\n",
              " ('a', 111293),\n",
              " ('and', 111161),\n",
              " ('of', 101207),\n",
              " ('to', 93119),\n",
              " ('is', 72976),\n",
              " ('in', 63110),\n",
              " ('i', 49427),\n",
              " ('this', 48747),\n",
              " ('that', 46589)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "TEXT.vocab.freqs.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YUH4hYZbhOv"
      },
      "source": [
        "### Creating the Iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz7LEdQlbhOw"
      },
      "source": [
        "During training, we'll be using a special kind of Iterator, called the **BucketIterator**. When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
        "\n",
        "e.g.\n",
        "\\[ \n",
        "\\[3, 15, 2, 7\\],\n",
        "\\[4, 1\\], \n",
        "\\[5, 5, 6, 8, 1\\] \n",
        "\\] -> \\[ \n",
        "\\[3, 15, 2, 7, **0**\\],\n",
        "\\[4, 1, **0**, **0**, **0**\\], \n",
        "\\[5, 5, 6, 8, 1\\] \n",
        "\\] \n",
        "\n",
        "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time. The BucketIterator groups sequences of similar lengths together for each batch to minimize padding.\n",
        "\n",
        "Complete the definition of the **BucketIterator** object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqHLMmftbhOx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a7d82b5-f38c-4513-f225-7d254c5a3347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ],
      "source": [
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "        (trn, vld, tst),\n",
        "        batch_sizes=(64, 64, 64),\n",
        "        sort=True,\n",
        "        sort_key=lambda x: len(x.text),\n",
        "        sort_within_batch=False,\n",
        "        device='cuda',\n",
        "        repeat=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNJwLDqObhOx"
      },
      "source": [
        "Let's take a look at what the output of the BucketIterator looks like. Do not be suprised **batch_first=True**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7DfA_ixbhOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b50e5ae-63d1-408f-9712-4e4245a2bdb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   10, 26916,  7270,  ...,    10,     9, 43905],\n",
              "        [   20,     2, 24459,  ...,     7,   364,     7],\n",
              "        [    7, 22208,     7,  ...,     3,     2,     3],\n",
              "        ...,\n",
              "        [    1,     1,     1,  ...,    25,   219,    89],\n",
              "        [    1,     1,     1,  ...,    40,   531,   139],\n",
              "        [    1,     1,     1,  ...,     9,   112,  5633]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "batch = next(train_iter.__iter__()); batch.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GprOp69wbhOy"
      },
      "source": [
        "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmogrKMUbhOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d73114ce-d961-4a8b-bcb6-11fa00f8155c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'text', 'label'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "batch.__dict__.keys()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.transpose(batch.text, 0, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTqOxnQcnVtp",
        "outputId": "4d08b222-71cf-435c-da0c-88c2c967b36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   10,    20,     7,  ...,     1,     1,     1],\n",
              "        [26916,     2, 22208,  ...,     1,     1,     1],\n",
              "        [ 7270, 24459,     7,  ...,     1,     1,     1],\n",
              "        ...,\n",
              "        [   10,     7,     3,  ...,    25,    40,     9],\n",
              "        [    9,   364,     2,  ...,   219,   531,   112],\n",
              "        [43905,     7,     3,  ...,    89,   139,  5633]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpS4jHCkbhOy"
      },
      "source": [
        "### Define the RNN-based text classification model (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SADMRATJbhOz"
      },
      "source": [
        "Start simple first. Implement the model according to the shema below.  \n",
        "![alt text](https://miro.medium.com/max/1396/1*v-tLYQCsni550A-hznS0mw.jpeg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CQLAGHEbhOz"
      },
      "outputs": [],
      "source": [
        "class RNNBaseline(nn.Module):\n",
        "    def __init__(self, V, D, emb_dim, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(V+1, emb_dim, padding_idx=1)\n",
        "\n",
        "        self.gru = nn.GRU(emb_dim, D, batch_first=True)\n",
        "        self.linear = nn.Linear(D, num_classes)\n",
        "        self.sm = nn.Sigmoid()\n",
        "            \n",
        "    def forward(self, seq):\n",
        "        seq = torch.transpose(seq, 0, 1)\n",
        "\n",
        "        x = self.embed(seq)\n",
        "        \n",
        "        prev, x = self.gru(x)\n",
        "        # print(x.shape)\n",
        "        x = self.linear(x)\n",
        "        preds = self.sm(x)\n",
        "        \n",
        "        preds = torch.squeeze(preds)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhvwlIYVbhOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6be72be9-bd04-456a-bc38-d29ff2ff5407"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNBaseline(\n",
              "  (embed): Embedding(202237, 200, padding_idx=1)\n",
              "  (gru): GRU(200, 300, batch_first=True)\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "em_sz = 200\n",
        "nh = 300\n",
        "model = RNNBaseline(vocab_size, nh, emb_dim=em_sz); model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhatfRTEbhO0"
      },
      "source": [
        "If you're using a GPU, remember to call model.cuda() to move your model to the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uBd1ZUZbhO0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d15e2f-8303-4da0-8313-7e118040f33d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNBaseline(\n",
              "  (embed): Embedding(202237, 200, padding_idx=1)\n",
              "  (gru): GRU(200, 300, batch_first=True)\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVX460AkbhO0"
      },
      "source": [
        "### The training loop (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRs7Ds6BbhO1"
      },
      "source": [
        "Define the optimization and the loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKMZEfumbhO1"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNBlhy0-bhO1"
      },
      "source": [
        "Define the stopping criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Была попытка обучить на 20 эпохах, но после 5-ти заметно переобучение модели. \n"
      ],
      "metadata": {
        "id": "nuHlAbNbua-q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTCphX4BbhO1"
      },
      "outputs": [],
      "source": [
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeDwezIDbhO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bdfb339-027d-4448-bdf6-6e089c204e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.010040745798179082, Validation Loss: 0.0095876127799352\n",
            "Epoch: 2, Training Loss: 0.00681613667522158, Validation Loss: 0.006512776052951813\n",
            "Epoch: 3, Training Loss: 0.0035694457711918015, Validation Loss: 0.007702694930632909\n",
            "Epoch: 4, Training Loss: 0.00240104661429567, Validation Loss: 0.006040231561660767\n",
            "Epoch: 5, Training Loss: 0.001051394115208781, Validation Loss: 0.007511228881279627\n",
            "CPU times: user 1min 3s, sys: 1.93 s, total: 1min 5s\n",
            "Wall time: 1min 7s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter: \n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)   \n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    for batch in val_iter:\n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x) \n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(pred, gt):\n",
        "    print(f'Accuracy: {accuracy_score(gt, pred):.2f}')\n",
        "    print(f'Precision: {precision_score(gt, pred):.2f}')\n",
        "    print(f'Recall: {recall_score(gt, pred):.2f}')\n",
        "    print(f'F1: {f1_score(gt, pred):.2f}')"
      ],
      "metadata": {
        "id": "mituXMC3j67u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLxfwaehbhO2"
      },
      "source": [
        "### Calculate performance of the trained model (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awB7798AbhO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cae02ece-8437-4bfc-e1e3-1013e4007843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84\n",
            "Precision: 0.79\n",
            "Recall: 0.89\n",
            "F1: 0.83\n"
          ]
        }
      ],
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjWVtHpcbhO2"
      },
      "source": [
        "Write down the calculated performance\n",
        "\n",
        "### Accuracy: 0.83\n",
        "### Precision: 0.78\n",
        "### Recall: 0.86\n",
        "### F1: 0.82"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1AjVz4CbhO2"
      },
      "source": [
        "### Experiments (10 points)\n",
        "\n",
        "Experiment with the model and achieve better results. You can find advices [here](https://arxiv.org/abs/1801.06146). Implement and describe your experiments in details, mention what was helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XBmaI28bhO2"
      },
      "source": [
        "### 1. Простая RNN с GRU но больше размерность эмбединга и размер скрытого слоя\n",
        "`em_sz = 300`\n",
        "\n",
        "`nh = 500`\n",
        "\n",
        "#### Accuracy: 0.81\n",
        "#### Precision: 0.95\n",
        "#### Recall: 0.74\n",
        "#### F1: 0.83\n",
        "\n",
        "### 2. RNN, но вместо GRU используется LSTM модуль\n",
        "\n",
        "#### Accuracy: 0.83\n",
        "#### Precision: 0.76\n",
        "#### Recall: 0.88\n",
        "#### F1: 0.82\n",
        "\n",
        "### 3. LSTM с предыдущего эксперимента, но добавлен механизм внимания (Self-Attention)\n",
        "Число эпох обучения увеличил вдвое `epochs = 10`\n",
        "#### Accuracy: 0.81\n",
        "#### Precision: 0.78\n",
        "#### Recall: 0.84\n",
        "#### F1: 0.81"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, V, D, emb_dim, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(V+1, emb_dim, padding_idx=1)\n",
        "\n",
        "        self.gru = nn.GRU(emb_dim, D, batch_first=True)\n",
        "        self.linear = nn.Linear(D, num_classes)\n",
        "        self.sm = nn.Sigmoid()\n",
        "            \n",
        "    def forward(self, seq):\n",
        "        seq = torch.transpose(seq, 0, 1)\n",
        "\n",
        "        x = self.embed(seq)\n",
        "        \n",
        "        prev, x = self.gru(x)\n",
        "        # print(x.shape)\n",
        "        x = self.linear(x)\n",
        "        preds = self.sm(x)\n",
        "        \n",
        "        preds = torch.squeeze(preds)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "MlQcQc_B195c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "em_sz = 300\n",
        "nh = 500\n",
        "model = RNN(vocab_size, nh, emb_dim=em_sz); model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GlqqPFh2D7X",
        "outputId": "2e3492a5-8085-496f-fe7f-965faa4b1f41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(202237, 300, padding_idx=1)\n",
              "  (gru): GRU(300, 500, batch_first=True)\n",
              "  (linear): Linear(in_features=500, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOyNRSzu2Udv",
        "outputId": "fbb72817-136b-4cfb-95b9-7ca5524837c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embed): Embedding(202237, 300, padding_idx=1)\n",
              "  (gru): GRU(300, 500, batch_first=True)\n",
              "  (linear): Linear(in_features=500, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCELoss()"
      ],
      "metadata": {
        "id": "Nk3Evpeb2VL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter: \n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)   \n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    for batch in val_iter:\n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x) \n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afgV2FD_2Z7o",
        "outputId": "8dadfa3f-fa8f-44cb-9142-24425fe4153a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.009736107305118015, Validation Loss: 0.0075226970473925275\n",
            "Epoch: 2, Training Loss: 0.0047743097926889145, Validation Loss: 0.005689547514915467\n",
            "Epoch: 3, Training Loss: 0.0019570412627288275, Validation Loss: 0.005847744688888391\n",
            "Epoch: 4, Training Loss: 0.0006602316673645483, Validation Loss: 0.008146367043256759\n",
            "Epoch: 5, Training Loss: 0.0003243293537475568, Validation Loss: 0.008822253026564916\n",
            "CPU times: user 1min 45s, sys: 5.85 s, total: 1min 51s\n",
            "Wall time: 1min 51s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvcs0yNO2dLH",
        "outputId": "b0480202-cc80-4d19-fb12-458a96186674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.85\n",
            "Precision: 0.89\n",
            "Recall: 0.82\n",
            "F1: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, V, D, emb_dim, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = D\n",
        "        self.embed = nn.Embedding(V+1, emb_dim, padding_idx=1)\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, D, batch_first=True)\n",
        "\n",
        "        self.linear = nn.Linear(D, num_classes)\n",
        "        self.sm = nn.Sigmoid()\n",
        "            \n",
        "    def forward(self, seq):\n",
        "        seq = torch.transpose(seq, 0, 1)\n",
        "\n",
        "        x = self.embed(seq)\n",
        "        \n",
        "        out, (x, _) = self.lstm(x)\n",
        "        # print(x.shape)\n",
        "        # out = out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        x = self.linear(x)\n",
        "        preds = self.sm(x)\n",
        "        \n",
        "        preds = torch.squeeze(preds)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "emeXtL3fNAcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "em_sz = 200\n",
        "nh = 300\n",
        "model = LSTM(vocab_size, nh, emb_dim=em_sz); model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BsZYsGBNmJx",
        "outputId": "b8275dd2-732d-4773-e087-e08074dc0fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embed): Embedding(202237, 200, padding_idx=1)\n",
              "  (lstm): LSTM(200, 300, batch_first=True)\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCW24BSwNoOc",
        "outputId": "45daf387-d10e-47b6-9436-407ebdcd75ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (embed): Embedding(202237, 200, padding_idx=1)\n",
              "  (lstm): LSTM(200, 300, batch_first=True)\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCELoss()"
      ],
      "metadata": {
        "id": "1rxMNr2_Nrqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter: \n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)   \n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    for batch in val_iter:\n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x) \n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N-gYsH0Nz_C",
        "outputId": "6c62c89d-aaef-4fb5-eef9-a8903baed35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.010147611509050641, Validation Loss: 0.009657022051016489\n",
            "Epoch: 2, Training Loss: 0.008146413929121835, Validation Loss: 0.008766460768381754\n",
            "Epoch: 3, Training Loss: 0.005623402918662344, Validation Loss: 0.009291505881150564\n",
            "Epoch: 4, Training Loss: 0.0036178770635809216, Validation Loss: 0.008916429054737092\n",
            "Epoch: 5, Training Loss: 0.002209438592089074, Validation Loss: 0.009596401166915893\n",
            "CPU times: user 1min 10s, sys: 2.65 s, total: 1min 12s\n",
            "Wall time: 1min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ],
      "metadata": {
        "id": "hIH5hsQSN0at",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4db137-92f0-4108-9396-ac6bd5202b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.72\n",
            "Precision: 0.77\n",
            "Recall: 0.71\n",
            "F1: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Attn(nn.Module):\n",
        "    def __init__(self, V, D, emb_dim, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = D\n",
        "        self.embed = nn.Embedding(V+1, emb_dim, padding_idx=1)\n",
        "\n",
        "        self.lstm = nn.LSTM(emb_dim, D, batch_first=True)\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(D, 1)\n",
        "        \n",
        "        self.linear = nn.Linear(D, num_classes)\n",
        "        self.sm = nn.Sigmoid()\n",
        "            \n",
        "    def forward(self, seq):\n",
        "        seq = torch.transpose(seq, 0, 1)\n",
        "\n",
        "        x = self.embed(seq)\n",
        "        \n",
        "        out, (x, _) = self.lstm(x)\n",
        "        # print(x.shape)\n",
        "        # out = out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        x, _ = self.attn(x, x, x)\n",
        "\n",
        "        x = self.linear(x)\n",
        "        preds = self.sm(x)\n",
        "        \n",
        "        preds = torch.squeeze(preds)\n",
        "        return preds"
      ],
      "metadata": {
        "id": "w7MRxYsg2x7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(TEXT.vocab)\n",
        "em_sz = 200\n",
        "nh = 300\n",
        "model = LSTM_Attn(vocab_size, nh, emb_dim=em_sz); model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksCfZh92zu1t",
        "outputId": "6761e546-4944-4360-e73e-5a5464eba417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_Attn(\n",
              "  (embed): Embedding(202237, 200, padding_idx=1)\n",
              "  (lstm): LSTM(200, 300, batch_first=True)\n",
              "  (attn): MultiheadAttention(\n",
              "    (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
              "  )\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4vTPA_U2o9B",
        "outputId": "981dc0e7-03d6-415e-d045-2e388a02f31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_Attn(\n",
              "  (embed): Embedding(202237, 200, padding_idx=1)\n",
              "  (lstm): LSTM(200, 300, batch_first=True)\n",
              "  (attn): MultiheadAttention(\n",
              "    (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
              "  )\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCELoss()"
      ],
      "metadata": {
        "id": "2NMMPhru2quX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter: \n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)   \n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    for batch in val_iter:\n",
        "        \n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x) \n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch2T8vNp2sD4",
        "outputId": "3530e910-9275-4b79-8a57-fa1d1087aaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.010146206368718828, Validation Loss: 0.010841795555750528\n",
            "Epoch: 2, Training Loss: 0.009751753304685865, Validation Loss: 0.008532008218765258\n",
            "Epoch: 3, Training Loss: 0.00543042129107884, Validation Loss: 0.005889902116854986\n",
            "Epoch: 4, Training Loss: 0.002367686459262456, Validation Loss: 0.007636993495623271\n",
            "Epoch: 5, Training Loss: 0.0015292037265375257, Validation Loss: 0.01303813465833664\n",
            "Epoch: 6, Training Loss: 0.0009754644131770224, Validation Loss: 0.00883774971763293\n",
            "Epoch: 7, Training Loss: 0.000580570675364288, Validation Loss: 0.022201929569244386\n",
            "Epoch: 8, Training Loss: 0.0005947379629218111, Validation Loss: 0.012605170675118764\n",
            "Epoch: 9, Training Loss: 0.0003124555932439762, Validation Loss: 0.03167486324310303\n",
            "Epoch: 10, Training Loss: 0.0003928777390600382, Validation Loss: 0.04041039793491363\n",
            "CPU times: user 2min 26s, sys: 2.08 s, total: 2min 28s\n",
            "Wall time: 2min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06-SEZRQ2uoU",
        "outputId": "52e8e8eb-1902-4894-d6a0-5e3f8bdaf2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.79\n",
            "Precision: 0.94\n",
            "Recall: 0.73\n",
            "F1: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hG3vUYhh53yU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Sentiment_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}