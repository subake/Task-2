{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7JmJTghG2bP"
      },
      "source": [
        "## Assignment 2.2: Text classification via CNN (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QPMrQE0G2bV"
      },
      "source": [
        "In this assignment you should perform sentiment analysis of the IMDB reviews based on CNN architecture. Read carefully [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf) by Yoon Kim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr9SojHZG2bW",
        "outputId": "ea15a411-61d0-4511-b78f-ecdf6f66a1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: torchtext==0.7 in /usr/local/lib/python3.7/dist-packages (0.7.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (0.1.96)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (1.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.7) (4.64.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.7) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.7) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.6.0\n",
        "!pip install torchtext==0.7\n",
        "!pip install numpy\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ust0oHmuG2bX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import datasets\n",
        "from torchtext.data import Field, LabelField\n",
        "from torchtext.data import Iterator\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwQTOkDtG2bY"
      },
      "source": [
        "### Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OllNO1iTG2bY",
        "outputId": "26faba52-dce5-4fd0-f67d-6e120244e3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ],
      "source": [
        "TEXT = Field(sequential=True, lower=True, batch_first=True)\n",
        "LABEL = LabelField(batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQgNY4HBG2bZ",
        "outputId": "1cc2db2a-7419-43c4-f704-8ab8f3bd2884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
          ]
        }
      ],
      "source": [
        "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
        "trn, vld = train.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lZo9WqicG2ba"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "TEXT.build_vocab(trn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aMiOVaZkG2bb"
      },
      "outputs": [],
      "source": [
        "LABEL.build_vocab(trn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L76HxbW7G2bc"
      },
      "source": [
        "### Creating the Iterator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4E996y0G2bc"
      },
      "source": [
        "Define an iterator here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaEhmzHqG2bc",
        "outputId": "c8c6f5f1-411b-4303-fad6-db80da8c67b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        }
      ],
      "source": [
        "train_iter, val_iter, test_iter = Iterator.splits((trn, vld, tst), \n",
        "                                                  batch_sizes=(64,64,64), \n",
        "                                                  sort_key=lambda x: len(x.text), \n",
        "                                                  device='cuda', \n",
        "                                                  sort=True,\n",
        "                                                  sort_within_batch=True,\n",
        "                                                  repeat=False\n",
        "                                                 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBWVYpjWG2bd"
      },
      "source": [
        "### Define CNN-based text classification model (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Т.к. длинна предложений разная, то есть идея: разворачивать двумерные вектора эмбеддингов в одномерные, но при этом увеличивать размер ядра, паддинг и страйд в D (размерность эмбеддинга слова) раз."
      ],
      "metadata": {
        "id": "q72lg7PMiXJ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IjKYuJKLG2bd"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, V, D, sent_length, kernel_sizes, num_classes, dropout=0.5, filters=100):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(V+1, D, padding_idx=1)\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        for kernel_size in kernel_sizes:\n",
        "          self.convs.append(nn.Conv1d(in_channels=1, out_channels=filters, kernel_size=kernel_size*D, padding=kernel_size//2*D, stride=D))\n",
        "\n",
        "        self.ac = nn.ReLU()\n",
        "        self.pool = F.max_pool1d\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(filters*len(kernel_sizes), num_classes)\n",
        "        self.sm = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.shape[0], 1, -1)\n",
        "        # print(x.shape)\n",
        "        x = [self.ac(conv(x)) for conv in self.convs]\n",
        "\n",
        "        x = torch.cat([self.pool(x_, x_.shape[2]).squeeze(2) for x_ in x], dim=1)\n",
        "\n",
        "        x = self.linear(self.dropout(x))\n",
        "        logit = self.sm(x)\n",
        "        # print(logit.shape)\n",
        "\n",
        "        return logit.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fG0nWSqLG2be"
      },
      "outputs": [],
      "source": [
        "kernel_sizes = [3,4,5]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dropout = 0.5\n",
        "dim = 300\n",
        "n = 2470\n",
        "\n",
        "model = CNN(vocab_size, dim, n, kernel_sizes, num_classes=1, dropout=dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aYXwAApAG2be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9539028a-6b5d-480a-e76a-7d760542074d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embed): Embedding(201524, 300, padding_idx=1)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv1d(1, 100, kernel_size=(900,), stride=(300,), padding=(300,))\n",
              "    (1): Conv1d(1, 100, kernel_size=(1200,), stride=(300,), padding=(600,))\n",
              "    (2): Conv1d(1, 100, kernel_size=(1500,), stride=(300,), padding=(600,))\n",
              "  )\n",
              "  (ac): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jKkFvMEG2bf"
      },
      "source": [
        "### The training loop (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qW9m7SpG2bf"
      },
      "source": [
        "Define the optimization function and the loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E6V6iwdhG2bf"
      },
      "outputs": [],
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Esehz6G2bf"
      },
      "source": [
        "Think carefully about the stopping criteria. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "72qY6CuRG2bg"
      },
      "outputs": [],
      "source": [
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n6_Oku03G2bg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b738a8b0-bf7c-4fdf-b3dd-e51969e049df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.010273323764119829, Validation Loss: 0.009711514965693157\n",
            "Epoch: 2, Training Loss: 0.009726892719949995, Validation Loss: 0.009583226561546325\n",
            "Epoch: 3, Training Loss: 0.009472737111364092, Validation Loss: 0.009250420832633973\n",
            "Epoch: 4, Training Loss: 0.00925515822342464, Validation Loss: 0.00915322651863098\n",
            "Epoch: 5, Training Loss: 0.009072249221801758, Validation Loss: 0.009301971423625946\n",
            "Epoch: 6, Training Loss: 0.008915570860249656, Validation Loss: 0.008996675491333008\n",
            "Epoch: 7, Training Loss: 0.008769469048295703, Validation Loss: 0.009068146514892577\n",
            "Epoch: 8, Training Loss: 0.008640025976725987, Validation Loss: 0.008946968531608581\n",
            "Epoch: 9, Training Loss: 0.008521641763619013, Validation Loss: 0.009055007334550221\n",
            "Epoch: 10, Training Loss: 0.008446630115168436, Validation Loss: 0.008988609270254772\n",
            "Epoch: 11, Training Loss: 0.008376587048598698, Validation Loss: 0.00922734846274058\n",
            "Epoch: 12, Training Loss: 0.008282399928569793, Validation Loss: 0.009211606840292612\n",
            "Epoch: 13, Training Loss: 0.008252507058211735, Validation Loss: 0.009338195391496022\n",
            "Epoch: 14, Training Loss: 0.008191475633212498, Validation Loss: 0.008978433736165364\n",
            "Epoch: 15, Training Loss: 0.008161841678619384, Validation Loss: 0.008907558540503183\n",
            "Epoch: 16, Training Loss: 0.008115394571849278, Validation Loss: 0.00901057900985082\n",
            "Epoch: 17, Training Loss: 0.008103114679881505, Validation Loss: 0.009058327635129293\n",
            "Epoch: 18, Training Loss: 0.008069004491397313, Validation Loss: 0.009172386499245962\n",
            "Epoch: 19, Training Loss: 0.008056091363089424, Validation Loss: 0.008951720662911732\n",
            "Epoch: 20, Training Loss: 0.008033416269506728, Validation Loss: 0.009193684156735738\n",
            "CPU times: user 4min 25s, sys: 1.24 s, total: 4min 26s\n",
            "Wall time: 4min 29s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter:         \n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)\n",
        "        # print(preds)\n",
        "        # print('=======================================')\n",
        "        # print(y)\n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0 \n",
        "    for batch in val_iter:\n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x)\n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    \n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWP4Qp1BG2bh"
      },
      "source": [
        "### Calculate performance of the trained model (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(pred, gt):\n",
        "    print(f'Accuracy: {accuracy_score(gt, pred):.2f}')\n",
        "    print(f'Precision: {precision_score(gt, pred):.2f}')\n",
        "    print(f'Recall: {recall_score(gt, pred):.2f}')\n",
        "    print(f'F1: {f1_score(gt, pred):.2f}')"
      ],
      "metadata": {
        "id": "tcidHam-8RPV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "df-EUqhtG2bh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8ba585-796b-407a-8672-177e167ec7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84\n",
            "Precision: 0.93\n",
            "Recall: 0.79\n",
            "F1: 0.85\n"
          ]
        }
      ],
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk1f_BRMG2bh"
      },
      "source": [
        "Write down the calculated performance\n",
        "\n",
        "### Accuracy: 0.83\n",
        "### Precision: 0.94\n",
        "### Recall: 0.77\n",
        "### F1: 0.84"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kVtk5H6G2bh"
      },
      "source": [
        "### Experiments (10 points)\n",
        "\n",
        "Experiment with the model and achieve better results. Implement and describe your experiments in details, mention what was helpful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djndybieG2bi"
      },
      "source": [
        "### 1. Добавил больше ядер\n",
        "`kernel_sizes = [3,4,5,6,7]`\n",
        "#### Accuracy: 0.86\n",
        "#### Precision: 0.89\n",
        "#### Recall: 0.83\n",
        "#### F1: 0.86\n",
        "### 2. Больше ядер + увеличил размер скрытого слоя + число фильтров в свертках\n",
        "`kernel_sizes = [3,4,5,6,7]`\n",
        "\n",
        "`dim = 500`\n",
        "\n",
        "`filters=150`\n",
        "#### Accuracy: 0.86\n",
        "#### Precision: 0.89\n",
        "#### Recall: 0.85\n",
        "#### F1: 0.87\n",
        "### 3. Добавил механизм внимания (Self-Attention)\n",
        "Работает нестабильно, метрики силно скачут\n",
        "#### Accuracy: 0.50\n",
        "#### Precision: 1.00\n",
        "#### Recall: 0.50\n",
        "#### F1: 0.67"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_sizes = [3,4,5,6,7]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dropout = 0.5\n",
        "dim = 300\n",
        "n = 2470\n",
        "\n",
        "model = CNN(vocab_size, dim, n, kernel_sizes, num_classes=1, dropout=dropout, filters=100)"
      ],
      "metadata": {
        "id": "Gnkow8q97XK2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzsPuMnW7b0m",
        "outputId": "99ed8c48-f217-4a50-9196-5f3510a0d53b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embed): Embedding(201524, 300, padding_idx=1)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv1d(1, 100, kernel_size=(900,), stride=(300,), padding=(300,))\n",
              "    (1): Conv1d(1, 100, kernel_size=(1200,), stride=(300,), padding=(600,))\n",
              "    (2): Conv1d(1, 100, kernel_size=(1500,), stride=(300,), padding=(600,))\n",
              "    (3): Conv1d(1, 100, kernel_size=(1800,), stride=(300,), padding=(900,))\n",
              "    (4): Conv1d(1, 100, kernel_size=(2100,), stride=(300,), padding=(900,))\n",
              "  )\n",
              "  (ac): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=500, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "Bnaza1_G7enj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter:         \n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)\n",
        "        # print(preds)\n",
        "        # print('=======================================')\n",
        "        # print(y)\n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0 \n",
        "    for batch in val_iter:\n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x)\n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    \n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7HaQvgP7y0f",
        "outputId": "aa1af03a-3d83-49d5-e350-4522529a14ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.010222412603242057, Validation Loss: 0.010115425745646158\n",
            "Epoch: 2, Training Loss: 0.00965090147767748, Validation Loss: 0.009374126116434733\n",
            "Epoch: 3, Training Loss: 0.009379771035058158, Validation Loss: 0.009380531819661458\n",
            "Epoch: 4, Training Loss: 0.00915208581515721, Validation Loss: 0.009241183737913768\n",
            "Epoch: 5, Training Loss: 0.008982938744340625, Validation Loss: 0.009085426104068756\n",
            "Epoch: 6, Training Loss: 0.008819983398914337, Validation Loss: 0.009032229089736938\n",
            "Epoch: 7, Training Loss: 0.008695067395482745, Validation Loss: 0.009321585726737976\n",
            "Epoch: 8, Training Loss: 0.008578485330513546, Validation Loss: 0.009006008557478587\n",
            "Epoch: 9, Training Loss: 0.00848745161635535, Validation Loss: 0.009030736911296845\n",
            "Epoch: 10, Training Loss: 0.008422586875302451, Validation Loss: 0.009055590105056762\n",
            "Epoch: 11, Training Loss: 0.008351166718346731, Validation Loss: 0.009323565800984701\n",
            "Epoch: 12, Training Loss: 0.00828600993837629, Validation Loss: 0.009085092262427012\n",
            "Epoch: 13, Training Loss: 0.008247860148974828, Validation Loss: 0.00898204178015391\n",
            "Epoch: 14, Training Loss: 0.008186637549740928, Validation Loss: 0.008962273784478506\n",
            "Epoch: 15, Training Loss: 0.008149603334495, Validation Loss: 0.009222363897164663\n",
            "Epoch: 16, Training Loss: 0.008135363335268838, Validation Loss: 0.009349957621097565\n",
            "Epoch: 17, Training Loss: 0.008105913300173623, Validation Loss: 0.00924831166267395\n",
            "Epoch: 18, Training Loss: 0.008088532258783069, Validation Loss: 0.009119555815060933\n",
            "Epoch: 19, Training Loss: 0.008082734329359872, Validation Loss: 0.009164412860075632\n",
            "Epoch: 20, Training Loss: 0.008064504516124725, Validation Loss: 0.008990933577219645\n",
            "CPU times: user 6min 21s, sys: 789 ms, total: 6min 22s\n",
            "Wall time: 6min 23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ],
      "metadata": {
        "id": "gyLOBhI474iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bae02d0-639c-4e19-c7a3-70e1a15e8ba4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n",
            "Precision: 0.89\n",
            "Recall: 0.83\n",
            "F1: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_sizes = [3,4,5,6,7]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dropout = 0.5\n",
        "dim = 500\n",
        "n = 2470\n",
        "\n",
        "model = CNN(vocab_size, dim, n, kernel_sizes, num_classes=1, dropout=dropout, filters=150)"
      ],
      "metadata": {
        "id": "rvXzAiV18L4G"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "id": "B5y3a3Pj8OeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6977ffc-1fc2-4ac3-e11f-c4a80bb4378e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embed): Embedding(201524, 500, padding_idx=1)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv1d(1, 150, kernel_size=(1500,), stride=(500,), padding=(500,))\n",
              "    (1): Conv1d(1, 150, kernel_size=(2000,), stride=(500,), padding=(1000,))\n",
              "    (2): Conv1d(1, 150, kernel_size=(2500,), stride=(500,), padding=(1000,))\n",
              "    (3): Conv1d(1, 150, kernel_size=(3000,), stride=(500,), padding=(1500,))\n",
              "    (4): Conv1d(1, 150, kernel_size=(3500,), stride=(500,), padding=(1500,))\n",
              "  )\n",
              "  (ac): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=750, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "Z-BSgTox8QRB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter:         \n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)\n",
        "        # print(preds)\n",
        "        # print('=======================================')\n",
        "        # print(y)\n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0 \n",
        "    for batch in val_iter:\n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x)\n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    \n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ],
      "metadata": {
        "id": "TWKZyd6a8SU_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0449130b-ac86-456b-b871-9b932c67b045"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.01019511307988848, Validation Loss: 0.009714871831734976\n",
            "Epoch: 2, Training Loss: 0.009716641112736294, Validation Loss: 0.009441871412595114\n",
            "Epoch: 3, Training Loss: 0.00948992839881352, Validation Loss: 0.009371318940321604\n",
            "Epoch: 4, Training Loss: 0.009354449023519243, Validation Loss: 0.009398946475982667\n",
            "Epoch: 5, Training Loss: 0.009223553809097835, Validation Loss: 0.009362363692124684\n",
            "Epoch: 6, Training Loss: 0.009088335469790867, Validation Loss: 0.009152149403095245\n",
            "Epoch: 7, Training Loss: 0.009054400174958366, Validation Loss: 0.009875938284397125\n",
            "Epoch: 8, Training Loss: 0.008933635129247393, Validation Loss: 0.009517063240210216\n",
            "Epoch: 9, Training Loss: 0.008871829991681235, Validation Loss: 0.009251532562573751\n",
            "Epoch: 10, Training Loss: 0.00879664854322161, Validation Loss: 0.009853382921218873\n",
            "Epoch: 11, Training Loss: 0.008780704346724918, Validation Loss: 0.009905289403597514\n",
            "Epoch: 12, Training Loss: 0.008697654153619493, Validation Loss: 0.009136239385604858\n",
            "Epoch: 13, Training Loss: 0.008654162716865539, Validation Loss: 0.009523029565811157\n",
            "Epoch: 14, Training Loss: 0.00858360959121159, Validation Loss: 0.009026534271240234\n",
            "Epoch: 15, Training Loss: 0.008602574976852961, Validation Loss: 0.00903160759607951\n",
            "Epoch: 16, Training Loss: 0.00851145258971623, Validation Loss: 0.009241327710946401\n",
            "Epoch: 17, Training Loss: 0.00853893757377352, Validation Loss: 0.008973837908109029\n",
            "Epoch: 18, Training Loss: 0.008469114245687212, Validation Loss: 0.009267713141441345\n",
            "Epoch: 19, Training Loss: 0.008449388349056244, Validation Loss: 0.009480676929155985\n",
            "Epoch: 20, Training Loss: 0.008403287262575967, Validation Loss: 0.00893320689201355\n",
            "CPU times: user 12min 43s, sys: 1.16 s, total: 12min 44s\n",
            "Wall time: 12min 46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ],
      "metadata": {
        "id": "ap-35a7U8UFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fad65700-b3b4-4615-e7ce-eaee0008ce98"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.86\n",
            "Precision: 0.89\n",
            "Recall: 0.85\n",
            "F1: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Attn(nn.Module):\n",
        "    def __init__(self, V, D, sent_length, kernel_sizes, num_classes, dropout=0.5, filters=100):\n",
        "        super(CNN_Attn, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(V+1, D, padding_idx=1)\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "\n",
        "        for kernel_size in kernel_sizes:\n",
        "          self.convs.append(nn.Conv1d(in_channels=1, out_channels=filters, kernel_size=kernel_size*D, padding=kernel_size//2*D, stride=D))\n",
        "\n",
        "        self.ac = nn.ReLU()\n",
        "        self.pool = F.max_pool1d        \n",
        "\n",
        "        self.attn = nn.MultiheadAttention(D, 1)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(filters*len(kernel_sizes), num_classes)\n",
        "        self.sm = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.shape[0], 1, -1)\n",
        "        # print(x.shape)\n",
        "        x = [self.ac(conv(x)) for conv in self.convs]\n",
        "\n",
        "        x = torch.cat([self.pool(x_, x_.shape[2]).squeeze(2) for x_ in x], dim=1)\n",
        "\n",
        "        x = torch.reshape(x, (1, x.shape[0], -1,))\n",
        "        # print(x.shape)        \n",
        "\n",
        "        x, _ = self.attn(x, x, x)\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.linear(self.dropout(x))\n",
        "        logit = self.sm(x)\n",
        "        # print(logit.shape)\n",
        "        return torch.reshape(logit, (-1,))"
      ],
      "metadata": {
        "id": "o2vMvGSa8HqJ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel_sizes = [3,4,5]\n",
        "vocab_size = len(TEXT.vocab)\n",
        "dropout = 0.5\n",
        "dim = 300\n",
        "n = 2470\n",
        "\n",
        "model = CNN_Attn(vocab_size, dim, n, kernel_sizes, num_classes=1, dropout=dropout)"
      ],
      "metadata": {
        "id": "RkPeoxnh8lHd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "metadata": {
        "id": "AfVHaqhB8pjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681456b4-d49b-439f-a21e-d640cec7fcd5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN_Attn(\n",
              "  (embed): Embedding(201524, 300, padding_idx=1)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv1d(1, 100, kernel_size=(900,), stride=(300,), padding=(300,))\n",
              "    (1): Conv1d(1, 100, kernel_size=(1200,), stride=(300,), padding=(600,))\n",
              "    (2): Conv1d(1, 100, kernel_size=(1500,), stride=(300,), padding=(600,))\n",
              "  )\n",
              "  (ac): ReLU()\n",
              "  (attn): MultiheadAttention(\n",
              "    (out_proj): _LinearWithBias(in_features=300, out_features=300, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
              "  (sm): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "loss_func = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "0mnEW75W8rdK"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for epoch in range(1, epochs + 1):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    model.train() \n",
        "    for batch in train_iter:         \n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        preds = model(x)\n",
        "        # print(preds)\n",
        "        # print('=======================================')\n",
        "        # print(y)\n",
        "        loss = loss_func(preds, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    epoch_loss = running_loss / len(trn)\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0 \n",
        "    for batch in val_iter:\n",
        "        \n",
        "        # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "        # x[:, :batch.text.shape[1]] = batch.text\n",
        "        # print(x)\n",
        "        # print(batch.text.shape[1])\n",
        "\n",
        "        x = batch.text\n",
        "        y = batch.label.float()\n",
        "        \n",
        "        preds = model(x)\n",
        "        loss = loss_func(preds, y)\n",
        "        val_loss += loss.item()\n",
        "        \n",
        "    val_loss /= len(vld)\n",
        "    \n",
        "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
      ],
      "metadata": {
        "id": "2sQq4g-I8wsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0567ccf7-e40b-46f1-b65f-ae62e5f1e425"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Training Loss: 0.0108502379826137, Validation Loss: 0.010905509614944457\n",
            "Epoch: 2, Training Loss: 0.01085207280090877, Validation Loss: 0.010905515670776368\n",
            "Epoch: 3, Training Loss: 0.010852704375130789, Validation Loss: 0.010905515670776368\n",
            "Epoch: 4, Training Loss: 0.010852704402378626, Validation Loss: 0.010905515670776368\n",
            "Epoch: 5, Training Loss: 0.010852704402378626, Validation Loss: 0.010905515670776368\n",
            "Epoch: 6, Training Loss: 0.010852704804284232, Validation Loss: 0.010905515670776368\n",
            "Epoch: 7, Training Loss: 0.010852704419408526, Validation Loss: 0.010905515670776368\n",
            "Epoch: 8, Training Loss: 0.010852704289981297, Validation Loss: 0.010905515670776368\n",
            "Epoch: 9, Training Loss: 0.010852704920087542, Validation Loss: 0.010905515670776368\n",
            "Epoch: 10, Training Loss: 0.010852704467092241, Validation Loss: 0.010905515670776368\n",
            "Epoch: 11, Training Loss: 0.01085270448071616, Validation Loss: 0.010905515670776368\n",
            "Epoch: 12, Training Loss: 0.010852704426220485, Validation Loss: 0.010905515670776368\n",
            "Epoch: 13, Training Loss: 0.010852704416002546, Validation Loss: 0.010905515670776368\n",
            "Epoch: 14, Training Loss: 0.010852704385348728, Validation Loss: 0.010905515670776368\n",
            "Epoch: 15, Training Loss: 0.010852704398972648, Validation Loss: 0.010905515670776368\n",
            "Epoch: 16, Training Loss: 0.010852704426220485, Validation Loss: 0.010905515670776368\n",
            "Epoch: 17, Training Loss: 0.010852704245703561, Validation Loss: 0.010905515670776368\n",
            "Epoch: 18, Training Loss: 0.012761014158385141, Validation Loss: 0.01264878850777944\n",
            "Epoch: 19, Training Loss: 0.012780674924169268, Validation Loss: 0.01264878850777944\n",
            "Epoch: 20, Training Loss: 0.012780674924169268, Validation Loss: 0.01264878850777944\n",
            "CPU times: user 4min 34s, sys: 662 ms, total: 4min 35s\n",
            "Wall time: 4min 36s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "gt = []\n",
        "\n",
        "for batch in test_iter:\n",
        "    # x = torch.ones(batch.text.shape[0], n, dtype=torch.int64).cuda()\n",
        "    # x[:, :batch.text.shape[1]] = batch.text\n",
        "    # print(x)\n",
        "    # print(batch.text.shape[1])\n",
        "\n",
        "    x = batch.text\n",
        "    y = batch.label.float()\n",
        "    \n",
        "    pred += model(x).round().tolist()\n",
        "    gt += y.tolist()\n",
        "\n",
        "get_metrics(gt, pred)"
      ],
      "metadata": {
        "id": "A8R_Y-7A8zLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9678b8-8347-433e-ac5b-8744bd1b1473"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
            "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.50\n",
            "Precision: 1.00\n",
            "Recall: 0.50\n",
            "F1: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KQZqB0A0iaKJ"
      },
      "execution_count": 35,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Sentiment_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}